% This file was created with Citavi 6.14.0.0

@online{.06.04.2014,
 year = {06.04.2014},
 title = {Road and Building Detection Datasets},
 url = {https://www.cs.toronto.edu/~vmnih/data/},
 urldate = {2022-10-06},
 abstract = {Massachusetts Roads Dataset



sollte so zitiert werden: 

@phdthesis{MnihThesis,

    author = {Volodymyr Mnih},

    title = {Machine Learning for Aerial Image Labeling},

    school = {University of Toronto},

    year = {2013}

}}
}


@online{db-ranking,
 year = {2023},
 author = {solid IT GmbH},
 title = {DBMS popularity broken down by database model - Popularity changes per category, April 2023},
 url = {https://db-engines.com/en/ranking_categories},
 urldate = {2023-04-06},
}


@online{.10.11.2022,
 year = {10.11.2022},
 title = {Index of /slides/2017},
 url = {http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf},
 urldate = {2022-11-10},
 abstract = {}
}


@online{.20.04.2022,
 year = {20.04.2022},
 title = {LandCover.ai},
 url = {https://landcover.ai.linuxpolska.com/},
 urldate = {2022-10-06},
 abstract = {}
}


@proceedings{.2018,
 year = {2018},
 title = {Proceedings of the 7th International Conference on Pattern Recognition Applications and Methods},
 publisher = {{SCITEPRESS - Science and Technology Publications}},
 isbn = {978-989-758-276-9},
 abstract = {},
 venue = {Funchal, Madeira, Portugal},
 eventdate = {16/01/2018 - 18/01/2018},
 eventtitle = {7th International Conference on Pattern Recognition Applications and Methods}
}


@proceedings{.2018b,
 year = {2018},
 title = {2018 IEEE Asia Pacific Conference on Circuits and Systems (APCCAS)},
 publisher = {IEEE},
 isbn = {978-1-5386-8240-1},
 abstract = {},
 venue = {Chengdu},
 eventdate = {26/10/2018 - 30/10/2018},
 eventtitle = {2018 IEEE Asia Pacific Conference on Circuits and Systems (APCCAS)}
}


@proceedings{.2018c,
 year = {2018},
 title = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
 isbn = {2160-7516},
 abstract = {},
 eventtitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}
}


@proceedings{.2021,
 year = {2021},
 title = {2020 25th International Conference on Pattern Recognition (ICPR)},
 isbn = {1051-4651},
 abstract = {},
 file = {2020 25th International Conference 2021:Attachments/2020 25th International Conference 2021.pdf:application/pdf},
 eventtitle = {2020 25th International Conference on Pattern Recognition (ICPR)}
}


@proceedings{.2022,
 year = {2022},
 abstract = {},
 eventtitle = {CARI 2022}
}


@online{.24.09.2022,
 year = {24.09.2022},
 title = {UAVid Semantic Segmentation Dataset},
 url = {https://uavid.nl/},
 urldate = {2022-10-06},
 abstract = {}
}


@misc{Amiri.19.02.2020,
 author = {Amiri, Mina and Brooks, Rupert and Rivaz, Hassan},
 year = {19.02.2020},
 title = {Fine tuning U-Net for ultrasound image segmentation: which layers?},
 url = {https://arxiv.org/pdf/2002.08438},
 abstract = {Fine-tuning a network which has been trained on a large dataset is an alternative to full training in order to overcome the problem of scarce and expensive data in medical applications. While the shallow layers of the network are usually kept unchanged, deeper layers are modified according to the new dataset. This approach may not work for ultrasound images due to their drastically different appearance. In this study, we investigated the effect of fine-tuning different layers of a U-Net which was trained on segmentation of natural images in breast ultrasound image segmentation. Tuning the contracting part and fixing the expanding part resulted in substantially better results compared to fixing the contracting part and tuning the expanding part. Furthermore, we showed that starting to fine-tune the U-Net from the shallow layers and gradually including more layers will lead to a better performance compared to fine-tuning the network from the deep layers moving back to shallow layers. We did not observe the same results on segmentation of X-ray images, which have different salient features compared to ultrasound, it may therefore be more appropriate to fine-tune the shallow layers rather than deep layers. Shallow layers learn lower level features (including speckle pattern, and probably the noise and artifact properties) which are critical in automatic segmentation in this modality.},
 file = {Amiri, Brooks et al. 19.02.2020 - Fine tuning U-Net for ultrasound:Attachments/Amiri, Brooks et al. 19.02.2020 - Fine tuning U-Net for ultrasound.pdf:application/pdf}
}


@article{Ankit.04.02.2019,
 author = {Ankit, Utkarsh},
 title = {Semantic Segmentation of Aerial Images Using Deep Learning},
 url = {https://towardsdatascience.com/semantic-segmentation-of-aerial-images-using-deep-learning-90fdf4ad780},
 urldate = {2022-10-06},
 journaltitle = {Towards Data Science},
 date = {2019-02-04},
 abstract = {Pixel-wise image segmentation is a challenging and demanding task in computer vision and image processing. This blog is about segmentation of Buildings from Aerial (satellite/drone) images$\ldots$},
 file = {Ankit 04.02.2019 - Semantic Segmentation of Aerial Images:Attachments/Ankit 04.02.2019 - Semantic Segmentation of Aerial Images.pdf:application/pdf}
}


@online{AshleshaVaidya.,
 author = {{Ashlesha Vaidya}},
 title = {Semi-Supervised Semantic Segmentation in UAV Imagery},
 url = {https://escholarship.org/uc/item/54z373nh#main},
 urldate = {2022-10-06},
 abstract = {},
 file = {eScholarship UC item 54z373nh:Attachments/eScholarship UC item 54z373nh.pdf:application/pdf}
}


@online{Ashwath.10.11.2020,
 author = {Ashwath, Balraj},
 year = {10.11.2020},
 title = {DeepGlobe Road Extraction Dataset},
 url = {https://www.kaggle.com/datasets/balraj98/deepglobe-road-extraction-dataset},
 urldate = {2022-10-06},
 abstract = {In disaster zones, especially in developing countries, maps and accessibility information are crucial for crisis response. [DeepGlobe Road Extraction Challenge ](https://competitions.codalab.org/competitions/18467) poses the challenge of automatically extracting roads and street networks from satellite images.

{\#}{\#}{\#} Acknowledgements

This dataset was obtained from [Road Extraction Challenge Track](https://competitions.codalab.org/competitions/18467) in [DeepGlobe Challenge](http://deepglobe.org/challenge.html) . For more details on the dataset refer the related publication - [DeepGlobe 2018: A Challenge to Parse the Earth through Satellite Images](https://arxiv.org/abs/1805.06561)

Any work based on the dataset should cite:

```

@InProceedings{DeepGlobe18,

author = {Demir, Ilke and Koperski, Krzysztof and Lindenbaum, David and Pang, Guan and Huang, Jing and Basu, Saikat and Hughes, Forest and Tuia, Devis and Raskar, Ramesh},

title = {DeepGlobe 2018: A Challenge to Parse the Earth Through Satellite Images},

booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},

month = {June},

year = {2018}

}

```

{\#}{\#}{\#} Terms {\&} Conditions

The [DeepGlobe Road Extraction Challenge](https://competitions.codalab.org/competitions/18467) and hence, the **dataset** are governed by [DeepGlobe Rules](http://deepglobe.org/docs/DeepGlobe{\_}Rules{\_}3{\_}2.pdf), [The DigitalGlobe's Internal Use License Agreement](http://deepglobe.org/docs/CVPR{\_}InternalUseLicenseAgreement{\_}07-11-18.pdf), and [Annotation License Agreement](http://deepglobe.org/docs/Annotation{\%}20License{\%}20Agreement.pdf).

{\#}{\#}{\#} Data

- The training data for Road Challenge contains 6226 satellite imagery in RGB, size 1024x1024.

- The imagery has 50cm pixel resolution, collected by DigitalGlobe's satellite.

- The dataset contains 1243 validation and 1101 test images (but no masks).

{\#}{\#}{\#} Label

- Each satellite image is paired with a mask image for road labels. The mask is a grayscale image, with white standing for road pixel, and black standing for background.

- File names for satellite images and the corresponding mask image are *id* {\_}sat.jpg and *id* {\_}mask.png. *id* is a randomized integer.

- Please note:

- The values of the mask image may not be pure 0 and 255. When converting to labels, please binarize them at threshold 128.

- The labels are not perfect due to the cost for annotating segmentation mask, specially in rural regions. In addition, we intentionally didn't annotate small roads within farmlands.}
}


@misc{Azimi.2018,
 author = {Azimi, Seyed Majid and Fischer, Peter and Korner, Marco and Reinartz, Peter},
 year = {2019},
 title = {Aerial LaneNet: Lane-Marking Semantic Segmentation in Aerial Imagery Using Wavelet-Enhanced Cost-Sensitive Symmetric Fully Convolutional Neural Networks},
 url = {https://arxiv.org/pdf/1803.06904},
 number = {5},
 abstract = {The knowledge about the placement and appearance of lane markings is a prerequisite for the creation of maps with high precision, necessary for autonomous driving, infrastructure monitoring, lane-wise traffic management, and urban planning. Lane markings are one of the important components of such maps. Lane markings convey the rules of roads to drivers. While these rules are learned by humans, an autonomous driving vehicle should be taught to learn them to localize itself. Therefore, accurate and reliable lane marking semantic segmentation in the imagery of roads and highways is needed to achieve such goals. We use airborne imagery which can capture a large area in a short period of time by introducing an aerial lane marking dataset. In this work, we propose a Symmetric Fully Convolutional Neural Network enhanced by Wavelet Transform in order to automatically carry out lane marking segmentation in aerial imagery. Due to a heavily unbalanced problem in terms of number of lane marking pixels compared with background pixels, we use a customized loss function as well as a new type of data augmentation step. We achieve a very high accuracy in pixel-wise localization of lane markings without using 3rd-party information. In this work, we introduce the first high-quality dataset used within our experiments which contains a broad range of situations and classes of lane markings representative of current transportation systems. This dataset will be publicly available and hence, it can be used as the benchmark dataset for future algorithms within this domain.},
 doi = {10.1109/TGRS.2018.2878510},
 file = {Azimi, Fischer et al. 2018 - Aerial LaneNet:Attachments/Azimi, Fischer et al. 2018 - Aerial LaneNet.pdf:application/pdf}
}


@article{balraj98.20.01.2021,
 author = {balraj98},
 title = {Road Extraction from Satellite Images [DeepLabV3+]},
 url = {https://www.kaggle.com/code/balraj98/road-extraction-from-satellite-images-deeplabv3},
 urldate = {2022-10-06},
 journaltitle = {Kaggle},
 date = {2021-01-20},
 abstract = {Explore and run machine learning code with Kaggle Notebooks | Using data from multiple data sources}
}


@misc{Behley.02.04.2019,
 author = {Behley, Jens and Garbade, Martin and Milioto, Andres and Quenzel, Jan and Behnke, Sven and Stachniss, Cyrill and Gall, Juergen},
 year = {02.04.2019},
 title = {SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences},
 url = {https://arxiv.org/pdf/1904.01416},
 abstract = {Semantic scene understanding is important for various applications. In particular, self-driving cars need a fine-grained understanding of the surfaces and objects in their vicinity. Light detection and ranging (LiDAR) provides precise geometric information about the environment and is thus a part of the sensor suites of almost all self-driving cars. Despite the relevance of semantic scene understanding for this application, there is a lack of a large dataset for this task which is based on an automotive LiDAR.  In this paper, we introduce a large dataset to propel research on laser-based semantic segmentation. We annotated all sequences of the KITTI Vision Odometry Benchmark and provide dense point-wise annotations for the complete {\$}360{\^{}}{o}{\$} field-of-view of the employed automotive LiDAR. We propose three benchmark tasks based on this dataset: (i) semantic segmentation of point clouds using a single scan, (ii) semantic segmentation using multiple past scans, and (iii) semantic scene completion, which requires to anticipate the semantic scene in the future. We provide baseline experiments and show that there is a need for more sophisticated models to efficiently tackle these tasks. Our dataset opens the door for the development of more advanced methods, but also provides plentiful data to investigate new research directions.},
 file = {Behley, Garbade et al. 02.04.2019 - SemanticKITTI:Attachments/Behley, Garbade et al. 02.04.2019 - SemanticKITTI.pdf:application/pdf},
 note = {ICCV2019. See teaser video at http://bit.ly/SemanticKITTI-teaser}
}


@misc{Boguszewski.05.05.2020,
 author = {Boguszewski, Adrian and Batorski, Dominik and Ziemba-Jankowska, Natalia and Dziedzic, Tomasz and Zambrzycka, Anna},
 year = {05.05.2020},
 title = {LandCover.ai: Dataset for Automatic Mapping of Buildings, Woodlands, Water and Roads from Aerial Imagery},
 url = {https://arxiv.org/pdf/2005.02264},
 abstract = {Monitoring of land cover and land use is crucial in natural resources management. Automatic visual mapping can carry enormous economic value for agriculture, forestry, or public administration. Satellite or aerial images combined with computer vision and deep learning enable precise assessment and can significantly speed up change detection. Aerial imagery usually provides images with much higher pixel resolution than satellite data allowing more detailed mapping. However, there is still a lack of aerial datasets made for the segmentation, covering rural areas with a resolution of tens centimeters per pixel, manual fine labels, and highly publicly important environmental instances like buildings, woods, water, or roads.  Here we introduce LandCover.ai (Land Cover from Aerial Imagery) dataset for semantic segmentation. We collected images of 216.27 sq. km rural areas across Poland, a country in Central Europe, 39.51 sq. km with resolution 50 cm per pixel and 176.76 sq. km with resolution 25 cm per pixel and manually fine annotated four following classes of objects: buildings, woodlands, water, and roads. Additionally, we report simple benchmark results, achieving 85.56{\%} of mean intersection over union on the test set. It proves that the automatic mapping of land cover is possible with a relatively small, cost-efficient, RGB-only dataset. The dataset is publicly available at https://landcover.ai.linuxpolska.com/},
 file = {Boguszewski, Batorski et al. 05.05.2020 - LandCover.ai:Attachments/Boguszewski, Batorski et al. 05.05.2020 - LandCover.ai.pdf:application/pdf}
}


@article{Brownlee.22.05.2018,
 author = {Brownlee, Jason},
 title = {A Gentle Introduction to k-fold Cross-Validation},
 url = {https://machinelearningmastery.com/k-fold-cross-validation/},
 urldate = {2022-10-07},
 journaltitle = {Machine Learning Mastery},
 date = {2018-05-22},
 abstract = {},
 file = {Brownlee 22.05.2018 - A Gentle Introduction to k-fold:Attachments/Brownlee 22.05.2018 - A Gentle Introduction to k-fold.pdf:application/pdf}
}


@inproceedings{C.Henry.2021,
 author = {{C. Henry} and {F. Fraundorfer} and {E. Vig}},
 title = {Aerial Road Segmentation in the Presence of Topological Label Noise},
 pages = {2336--2343},
 bookpagination = {page},
 isbn = {1051-4651},
 booktitle = {2020 25th International Conference on Pattern Recognition (ICPR)},
 year = {2021},
 abstract = {The availability of large-scale annotated datasets has enabled Fully-Convolutional Neural Networks to reach outstanding performance on road extraction in aerial images. However, high-quality pixel-level annotation is expensive to produce and even manually labeled data often contains topological errors. Trading off quality for quantity, many datasets rely on already available yet noisy labels, for example from OpenStreetMap. In this paper, we explore the training of custom U-Nets built with ResNet and DenseNet backbones using noise-aware losses that are robust towards label omission and registration noise. We perform an extensive evaluation of standard and noise-aware losses, including a novel Bootstrapped DICE-Coefficient loss, on two challenging road segmentation benchmarks. Our losses yield a consistent improvement in overall extraction quality and exhibit a strong capacity to cope with severe label noise. Our method generalizes well to two other fine-grained topology delineation tasks: surface crack detection for quality inspection and cell membrane extraction in electron microscopy imagery.},
 doi = {10.1109/ICPR48806.2021.9412054},
 file = {C. Henry, F. Fraundorfer et al. 2021 - Aerial Road Segmentation:Attachments/C. Henry, F. Fraundorfer et al. 2021 - Aerial Road Segmentation.pdf:application/pdf},
 eventtitle = {2020 25th International Conference on Pattern Recognition (ICPR)}
}


@misc{Cheng.05.10.2021,
 author = {Cheng, Dorothy and Lam, Edmund Y.},
 year = {05.10.2021},
 title = {Transfer Learning U-Net Deep Learning for Lung Ultrasound Segmentation},
 url = {https://arxiv.org/pdf/2110.02196},
 abstract = {Transfer learning (TL) for medical image segmentation helps deep learning models achieve more accurate performances when there are scarce medical images. This study focuses on completing segmentation of the ribs from lung ultrasound images and finding the best TL technique with U-Net, a convolutional neural network for precise and fast image segmentation. Two approaches of TL were used, using a pre-trained VGG16 model to build the U-Net (V-Unet) and pre-training U-Net network with grayscale natural salient object dataset (X-Unet). Visual results and roll coefficients (DICE) of the models were compared. X-Unet showed more accurate and artifact-free visual performances on the actual mask prediction, despite its lower DICE than V-Unet. A partial-frozen network fine-tuning (FT) technique was also applied to X-Unet to compare results between different FT strategies, which FT all layers slightly outperformed freezing part of the network. The effect of dataset sizes was also evaluated, showing the importance of the combination between TL and data augmentation.},
 pagetotal = {14},
 file = {Cheng, Lam 05.10.2021 - Transfer Learning U-Net Deep Learning:Attachments/Cheng, Lam 05.10.2021 - Transfer Learning U-Net Deep Learning.pdf:application/pdf},
 note = {14 pages, 8 figures}
}


@online{ChesapeakeConservancy.02.06.2022,
 author = {{Chesapeake Conservancy}},
 year = {02.06.2022},
 title = {Chesapeake Bay Program Land Use/Land Cover Data Project},
 url = {https://www.chesapeakeconservancy.org/conservation-innovation-center/high-resolution-data/lulc-data-project-2022/},
 urldate = {2022-10-06},
 abstract = {}
}


@book{ChristianWiedemann.1998,
 author = {{Christian Wiedemann} and {Christian Heipke} and {Helmut Mayer} and {Olivier Jamet}},
 year = {1998},
 title = {Empirical Evaluation Of Automatically Extracted Road Axes},
 abstract = {PDF | Internal self-diagnosis and external evaluation of the obtained results are of major importance for the relevance of any automatic system for... | Find, read and cite all the research you need on ResearchGate},
 file = {Christian Wiedemann, Christian Heipke et al. 1998 - Empirical Evaluation Of Automatically Extracted (2):Attachments/Christian Wiedemann, Christian Heipke et al. 1998 - Empirical Evaluation Of Automatically Extracted (2).pdf:application/pdf}
}


@misc{Clevert.23112015,
 author = {Clevert, Djork-Arn{\'e} and Unterthiner, Thomas and Hochreiter, Sepp},
 year = {23/11/2015},
 title = {Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)},
 url = {https://arxiv.org/pdf/1511.07289},
 abstract = {We introduce the {\textquotedbl}exponential linear unit{\textquotedbl} (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10{\%} classification error for a single crop, single model network.},
 file = {Clevert, Unterthiner et al. 23 11 2015 - Fast and Accurate Deep Network:Attachments/Clevert, Unterthiner et al. 23 11 2015 - Fast and Accurate Deep Network.pdf:application/pdf},
 note = {Published as a conference paper at ICLR 2016}
}


@inproceedings{Constantin.2018,
 author = {Constantin, Alexandre and Ding, Jian-Jiun and Lee, Yih-Cherng},
 title = {Accurate Road Detection from Satellite Images Using Modified U-net},
 pages = {423--426},
 bookpagination = {page},
 publisher = {IEEE},
 isbn = {978-1-5386-8240-1},
 booktitle = {2018 IEEE Asia Pacific Conference on Circuits and Systems (APCCAS)},
 year = {2018},
 abstract = {},
 doi = {10.1109/APCCAS.2018.8605652},
 eventtitle = {2018 IEEE Asia Pacific Conference on Circuits and Systems (APCCAS)},
 venue = {Chengdu},
 eventdate = {26/10/2018 - 30/10/2018}
}


@book{Cybenko.1999,
 author = {Cybenko, George and O'Leary, Dianne P. and Rissanen, Jorma},
 year = {1999},
 title = {The mathematics of information coding, extraction, and distribution},
 url = {http://www.springer.com/gb/   BLDSS},
 volume = {107},
 publisher = {Springer},
 isbn = {0387986650},
 location = {New York},
 series = {The IMA volumes in mathematics and its applications},
 abstract = {},
 organization = {{University of Minnesota. Institute for Mathematics and Its Applications}}
}


@article{Dice.1945,
 author = {Dice, Lee R.},
 year = {1945},
 title = {Measures of the Amount of Ecologic Association Between Species},
 pages = {297--302},
 pagination = {page},
 volume = {26},
 issn = {00129658},
 journaltitle = {Ecology},
 doi = {10.2307/1932409},
 number = {3},
 abstract = {}
}


@online{Englich.06.10.2022,
 author = {Englich, Markus},
 year = {06.10.2022},
 title = {Benchmark on Semantic Labeling},
 url = {https://www.isprs.org/education/benchmarks/UrbanSemLab/default.aspx},
 urldate = {2022-10-06},
 abstract = {Website of ISPRS - International Society for Photogrammetry and Remote Sensing}
}


@online{Englich.06.10.2022b,
 author = {Englich, Markus},
 year = {06.10.2022},
 title = {Detection and Reconstruction},
 url = {https://www.isprs.org/education/benchmarks/UrbanSemLab/detection-and-reconstruction.aspx#VaihigenDataDescr},
 urldate = {2022-10-06},
 abstract = {Website of ISPRS - International Society for Photogrammetry and Remote Sensing}
}


@online{Englich.17.11.2022,
 author = {Englich, Markus},
 year = {17.11.2022},
 title = {2D Semantic Labeling},
 url = {https://www.isprs.org/education/benchmarks/UrbanSemLab/semantic-labeling.aspx},
 urldate = {2022-11-17},
 abstract = {Website of ISPRS - International Society for Photogrammetry and Remote Sensing}
}


@online{Englich.17.11.2022b,
 author = {Englich, Markus},
 year = {17.11.2022},
 title = {2D Semantic Labeling Contest - Potsdam},
 url = {https://www.isprs.org/education/benchmarks/UrbanSemLab/2d-sem-label-potsdam.aspx},
 urldate = {2022-11-17},
 abstract = {Website of ISPRS - International Society for Photogrammetry and Remote Sensing}
}


@inproceedings{FaridaBintAhmadNchare.2022,
 author = {{Farida Bint Ahmad Nchare} and {Hippolyte Tapamo}},
 title = {Semantic segmentation of high-resolution aerial imagery using a fully convolutional network},
 url = {https://hal.inria.fr/hal-03715809},
 urldate = {2022-07-06},
 year = {2022},
 abstract = {Semantic segmentation applied to aerial imagery allows the extraction of terrestrial objects such as roads, buildings and even vegetation. Having large, detailed datasets of navigable roads, is of paramount importance in several application fields; namely urban planning, automatic navigation, disaster management. To reach this goal, extracting all roads in a given territory area is the first step. This paper presents a modern method to semantically segment aerial images for a road network extraction. We employ an encoder-decoder architecture to approach the problem of disconnected road regions faced by some existing methods. Using an FCN approach, the localization information was combined to the semantic one, to enable the reconstruction of the road by the proposed model, while being consistent with following the spatial alignment. The method was implemented and evaluated on the public dataset Massassuchets Roads. Results appear to be in full agreement with the theorical predictions and a significant improvement in road connectivity over some previous works; the proposed network achieved a precision of 87.86{\%} and a recall of 87.89{\%}.},
 file = {Farida Bint Ahmad Nchare, Hippolyte Tapamo 2022 - Semantic segmentation of high-resolution aerial:Attachments/Farida Bint Ahmad Nchare, Hippolyte Tapamo 2022 - Semantic segmentation of high-resolution aerial.pdf:application/pdf},
 language = {en},
 eventtitle = {CARI 2022}
}


@article{Fletcher.2018,
 author = {Fletcher, Sam and Islam, Md Zahidul},
 year = {2018},
 title = {Comparing sets of patterns with the Jaccard index},
 url = {http://journal.acs.org.au/index.php/ajis/article/view/1538},
 volume = {22},
 issn = {1449-8618},
 journaltitle = {Australasian Journal of Information Systems},
 shortjournal = {1},
 language = {en},
 doi = {10.3127/ajis.v22i0.1538},
 abstract = {The Australasian Journal of Information Systems is a refereed journal that publishes articles contributing to Information Systems theory and practice.},
 file = {Fletcher, Islam 2018 - Comparing sets of patterns:Attachments/Fletcher, Islam 2018 - Comparing sets of patterns.pdf:application/pdf}
}


@article{Gao.2019,
 author = {Gao, Lin and Song, Weidong and Dai, Jiguang and Chen, Yang},
 year = {2019},
 title = {Road Extraction from High-Resolution Remote Sensing Imagery Using Refined Deep Residual Convolutional Neural Network},
 pages = {552},
 pagination = {page},
 volume = {11},
 journaltitle = {Remote Sensing},
 doi = {10.3390/rs11050552},
 number = {5},
 abstract = {},
 file = {Gao, Song et al. 2019 - Road Extraction from High-Resolution Remote:Attachments/Gao, Song et al. 2019 - Road Extraction from High-Resolution Remote.pdf:application/pdf},
 note = {PII:  rs11050552}
}


@article{Geiger.2013,
 author = {Geiger, A. and Lenz, P. and Stiller, C. and Urtasun, R.},
 year = {2013},
 title = {Vision meets robotics: The KITTI dataset},
 pages = {1231--1237},
 pagination = {page},
 volume = {32},
 issn = {0278-3649},
 journaltitle = {The International Journal of Robotics Research},
 doi = {10.1177/0278364913491297},
 number = {11},
 abstract = {},
 file = {Geiger, Lenz et al. 2013 - Vision meets robotics:Attachments/Geiger, Lenz et al. 2013 - Vision meets robotics.pdf:application/pdf}
}


@online{GitHub.06.10.2022,
 author = {GitHub},
 year = {06.10.2022},
 title = {ayushdabra/drone-images-semantic-segmentation: Multi-class semantic segmentation performed on {\textquotedbl}Semantic Drone Dataset.{\textquotedbl}},
 url = {https://github.com/ayushdabra/drone-images-semantic-segmentation},
 urldate = {2022-10-06},
 abstract = {Multi-class semantic segmentation performed on {\textquotedbl}Semantic Drone Dataset.{\textquotedbl} - ayushdabra/drone-images-semantic-segmentation: Multi-class semantic segmentation performed on {\textquotedbl}Semantic Drone Dataset.{\textquotedbl}}
}


@online{GitHub.06.10.2022b,
 author = {GitHub},
 year = {06.10.2022},
 title = {mahmoudmohsen213/airs: Road Segmentation in Satellite Aerial Images},
 url = {https://github.com/mahmoudmohsen213/airs},
 urldate = {2022-10-06},
 abstract = {Road Segmentation in Satellite Aerial Images. Contribute to mahmoudmohsen213/airs development by creating an account on GitHub.}
}


@online{GitHub.07.10.2022,
 author = {GitHub},
 year = {07.10.2022},
 title = {dorltcheng/Transfer-Learning-U-Net-Deep-Learning-for-Lung-Ultrasound-Segmentation},
 url = {https://github.com/dorltcheng/Transfer-Learning-U-Net-Deep-Learning-for-Lung-Ultrasound-Segmentation},
 urldate = {2022-10-07},
 abstract = {Contribute to dorltcheng/Transfer-Learning-U-Net-Deep-Learning-for-Lung-Ultrasound-Segmentation development by creating an account on GitHub.}
}


@online{GitHub.07.10.2022b,
 author = {GitHub},
 year = {07.10.2022},
 title = {qubvel/segmentation{\_}models: Segmentation models with pretrained backbones. Keras and TensorFlow Keras},
 url = {https://github.com/qubvel/segmentation_models},
 urldate = {2022-10-07},
 abstract = {Segmentation models with pretrained backbones. Keras and TensorFlow Keras. - qubvel/segmentation{\_}models: Segmentation models with pretrained backbones. Keras and TensorFlow Keras.}
}


@book{Goodfellow.2016,
 author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
 year = {2016},
 title = {Deep learning},
 publisher = {{The MIT Press}},
 isbn = {9780262035613},
 location = {Cambridge, Massachusetts},
 series = {Adaptive computation and machine learning},
 abstract = {}
}


@misc{He.10122015,
 author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
 year = {10/12/2015},
 title = {Deep Residual Learning for Image Recognition},
 url = {https://arxiv.org/pdf/1512.03385},
 abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.  The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
 file = {He, Zhang et al. 10 12 2015 - Deep Residual Learning for Image:Attachments/He, Zhang et al. 10 12 2015 - Deep Residual Learning for Image.pdf:application/pdf},
 note = {Tech report}
}


@misc{Huang.25082016,
 author = {Huang, Gao and Liu, Zhuang and {van der Maaten}, Laurens and Weinberger, Kilian Q.},
 year = {25/08/2016},
 title = {Densely Connected Convolutional Networks},
 url = {https://arxiv.org/pdf/1608.06993},
 abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
 file = {Huang, Liu et al. 25 08 2016 - Densely Connected Convolutional Networks:Attachments/Huang, Liu et al. 25 08 2016 - Densely Connected Convolutional Networks.pdf:application/pdf},
 note = {CVPR 2017}
}


@online{HumansInTheLoop.29.05.2020,
 author = {{Humans In The Loop}},
 year = {29.05.2020},
 title = {Semantic segmentation of aerial imagery},
 url = {https://www.kaggle.com/datasets/humansintheloop/semantic-segmentation-of-aerial-imagery},
 urldate = {2022-10-06},
 abstract = {{\#}{\#}{\#} Context

Humans in the Loop is publishing an open access dataset annotated for a joint project with the Mohammed Bin Rashid Space Center in Dubai, the UAE.

{\#}{\#}{\#} Content

The dataset consists of aerial imagery of Dubai obtained by MBRSC satellites and annotated with pixel-wise semantic segmentation in 6 classes. The total volume of the dataset is 72 images grouped into 6 larger tiles. The classes are:

1. Building: {\#}3C1098

2. Land (unpaved area): {\#}8429F6

3. Road: {\#}6EC1E4

4. Vegetation: {\#}FEDD3A

5. Water: {\#}E2A929

6. Unlabeled: {\#}9B9B9B

{\#}{\#}{\#} Acknowledgements

The images were segmented by the trainees of the Roia Foundation in Syria.}
}


@misc{Ioffe.11022015,
 author = {Ioffe, Sergey and Szegedy, Christian},
 year = {11/02/2015},
 title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
 url = {https://arxiv.org/pdf/1502.03167},
 abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the accuracy of human raters.},
 file = {Ioffe, Szegedy 11 02 2015 - Batch Normalization:Attachments/Ioffe, Szegedy 11 02 2015 - Batch Normalization.pdf:application/pdf}
}


@misc{Kaiser.2017,
 author = {Kaiser, Pascal and Wegner, Jan Dirk and Lucchi, Aurelien and Jaggi, Martin and Hofmann, Thomas and Schindler, Konrad},
 year = {2017},
 title = {Learning Aerial Image Segmentation From Online Maps},
 url = {https://arxiv.org/pdf/1707.06879},
 number = {11},
 abstract = {This study deals with semantic segmentation of high-resolution (aerial) images where a semantic class label is assigned to each pixel via supervised classification as a basis for automatic map generation. Recently, deep convolutional neural networks (CNNs) have shown impressive performance and have quickly become the de-facto standard for semantic segmentation, with the added benefit that task-specific feature design is no longer necessary. However, a major downside of deep learning methods is that they are extremely data-hungry, thus aggravating the perennial bottleneck of supervised classification, to obtain enough annotated training data. On the other hand, it has been observed that they are rather robust against noise in the training labels. This opens up the intriguing possibility to avoid annotating huge amounts of training data, and instead train the classifier from existing legacy data or crowd-sourced maps which can exhibit high levels of noise. The question addressed in this paper is: can training with large-scale, publicly available labels replace a substantial part of the manual labeling effort and still achieve sufficient performance? Such data will inevitably contain a significant portion of errors, but in return virtually unlimited quantities of it are available in larger parts of the world. We adapt a state-of-the-art CNN architecture for semantic segmentation of buildings and roads in aerial images, and compare its performance when using different training data sets, ranging from manually labeled, pixel-accurate ground truth of the same city to automatic training data derived from OpenStreetMap data from distant locations. We report our results that indicate that satisfying performance can be obtained with significantly less manual annotation effort, by exploiting noisy large-scale training data.},
 doi = {10.1109/TGRS.2017.2719738},
 file = {Kaiser, Wegner et al. 2017 - Learning Aerial Image Segmentation:Attachments/Kaiser, Wegner et al. 2017 - Learning Aerial Image Segmentation.pdf:application/pdf}
}


@inproceedings{Kamiya.2018,
 author = {Kamiya, Ryosuke and Hotta, Kazuhiro and Oda, Kazuo and Kakuta, Satomi},
 title = {Road Detection from Satellite Images by Improving U-Net with Difference of Features},
 pages = {603--607},
 bookpagination = {page},
 publisher = {{SCITEPRESS - Science and Technology Publications}},
 isbn = {978-989-758-276-9},
 booktitle = {Proceedings of the 7th International Conference on Pattern Recognition Applications and Methods},
 year = {2018},
 abstract = {},
 doi = {10.5220/0006717506030607},
 eventtitle = {7th International Conference on Pattern Recognition Applications and Methods},
 venue = {Funchal, Madeira, Portugal},
 eventdate = {16/01/2018 - 18/01/2018}
}


@article{Kovan.05.01.2022,
 author = {Kovan, Ibrahim},
 title = {Implementing U-Net Architecture from scratch, What is U-Net? U-Net with Transfer Learning | Towards Data Science},
 url = {https://towardsdatascience.com/semantic-segmentation-of-aerial-imagery-captured-by-a-drone-using-different-u-net-approaches-91e32c92803c},
 urldate = {2022-10-06},
 journaltitle = {Towards Data Science},
 date = {2022-01-05},
 abstract = {This article explains the U-Net architecture and includes a real-world project with python implementation. Different coding approaches in computer vision (from scratch and transfer learning). Jaccard Index for computer vision.},
 file = {Kovan 05.01.2022 - Implementing U-Net Architecture from scratch:Attachments/Kovan 05.01.2022 - Implementing U-Net Architecture from scratch.pdf:application/pdf}
}


@book{Murphy.2012,
 author = {Murphy, Kevin P.},
 year = {2012},
 title = {Machine learning},
 publisher = {{MIT Press}},
 isbn = {9780262018029},
 subtitle = {A probabilistic perspective / Kevin P. Murphy},
 location = {Cambridge, Mass. and London},
 series = {Adaptive computation and machine learning series},
 abstract = {}
}


@online{Nithish.17.04.2022,
 author = {Nithish},
 year = {17.04.2022},
 title = {Satellite Imagery Road Segmentation},
 url = {https://medium.com/@nithishmailme/satellite-imagery-road-segmentation-ad2964dc3812},
 urldate = {2022-10-06},
 abstract = {},
 file = {Nithish 17.04.2022 - Satellite Imagery Road Segmentation:Attachments/Nithish 17.04.2022 - Satellite Imagery Road Segmentation.pdf:application/pdf}
}


@article{NitishSrivastava.2014,
 author = {{Nitish Srivastava} and {Geoffrey Hinton} and {Alex Krizhevsky} and {Ilya Sutskever} and {Ruslan Salakhutdinov}},
 year = {2014},
 title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
 url = {https://www.researchgate.net/publication/286794765_Dropout_A_Simple_Way_to_Prevent_Neural_Networks_from_Overfitting},
 pages = {1929--1958},
 pagination = {page},
 volume = {15},
 issn = {1532-4435},
 journaltitle = {Journal of Machine Learning Research},
 number = {1},
 abstract = {Request PDF | Dropout: A Simple Way to Prevent Neural Networks from Overfitting | Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such... | Find, read and cite all the research you need on ResearchGate}
}


@inproceedings{O.Filin.2018,
 author = {{O. Filin} and {A. Zapara} and {S. Panchenko}},
 title = {Road Detection with EOSResUNet and Post Vectorizing Algorithm},
 pages = {201--2014},
 bookpagination = {page},
 isbn = {2160-7516},
 booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
 year = {2018},
 abstract = {Object recognition on the satellite images is one of the most relevant and popular topics in the problem of pattern recognition. This was facilitated by many factors, such as a high number of satellites with high-resolution imagery, the significant development of computer vision, especially with a major breakthrough in the field of convolutional neural networks, a wide range of industry verticals for usage and still a quite empty market. Roads are one of the most popular objects for recognition. In this article, we want to present you the combination of work of neural network and postprocessing algorithm, due to which we get not only the coverage mask but also the vectors of all of the individual roads that are present in the image and can be used to address the higher-level tasks in the future. This approach was used to solve the DeepGlobe Road Extraction Challenge.},
 doi = {10.1109/CVPRW.2018.00036},
 eventtitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}
}


@article{Paul.18.12.2019,
 author = {Paul, Jerin},
 title = {Segmentation of Roads in Aerial Images. - Towards Data Science},
 url = {https://towardsdatascience.com/road-segmentation-727fb41c51af},
 urldate = {2022-10-06},
 journaltitle = {Towards Data Science},
 date = {2019-12-18},
 abstract = {This comprehensive article will help you to create a road segmentation model, which can detect and segment roads in aerial images.},
 file = {Paul 18.12.2019 - Segmentation of Roads in Aerial:Attachments/Paul 18.12.2019 - Segmentation of Roads in Aerial.pdf:application/pdf}
}


@article{Rasidin.28.07.2020,
 author = {Rasidin, Said},
 title = {Drone Aerial View Segmentation - Analytics Vidhya - Medium},
 url = {https://medium.com/analytics-vidhya/drone-aerial-view-segmentation-44046ff003b5},
 urldate = {2022-10-06},
 journaltitle = {Analytics Vidhya},
 date = {2020-07-28},
 abstract = {Drone uses already gain popularity in the past few years, it provides high resolution images compare to satellite imagery with lower cost, flexibility and low-flying altitude thus leading to$\ldots$},
 file = {Rasidin 28.07.2020 - Drone Aerial View Segmentation:Attachments/Rasidin 28.07.2020 - Drone Aerial View Segmentation.pdf:application/pdf}
}


@misc{Ronneberger.18052015,
 author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
 year = {18/05/2015},
 title = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
 url = {https://arxiv.org/pdf/1505.04597},
 abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
 file = {Ronneberger, Fischer et al. 18 05 2015 - U-Net Convolutional Networks for Biomedical:Attachments/Ronneberger, Fischer et al. 18 05 2015 - U-Net Convolutional Networks for Biomedical.pdf:application/pdf},
 note = {conditionally accepted at MICCAI 2015}
}


@article{Ruder.3212017,
 author = {Ruder, Sebastian},
 title = {Transfer Learning - Machine Learning's Next Frontier},
 url = {https://ruder.io/transfer-learning/},
 urldate = {2022-11-07},
 journaltitle = {Sebastian Ruder},
 date = {3/21/2017},
 abstract = {Deep learning models excel at learning from a large number of labeled examples, but typically do not generalize to conditions not seen during training. This post gives an overview of transfer learning, motivates why it warrants our application, and discusses practical applications and methods.},
 file = {Ruder 3 21 2017 - Transfer Learning:Attachments/Ruder 3 21 2017 - Transfer Learning.pdf:application/pdf}
}


@article{Sharma.21.08.2019,
 author = {Sharma, Pulkit},
 title = {Image Classification vs. Object Detection vs. Image Segmentation},
 url = {https://medium.com/analytics-vidhya/image-classification-vs-object-detection-vs-image-segmentation-f36db85fe81},
 urldate = {2022-11-03},
 journaltitle = {Analytics Vidhya},
 date = {2019-08-21},
 abstract = {The difference between Image Classification, Object Detection and Image Segmentation in the context of Computer Vision},
 file = {Sharma 21.08.2019 - Image Classification vs:Attachments/Sharma 21.08.2019 - Image Classification vs.pdf:application/pdf}
}


@misc{Simonyan.04092014,
 author = {Simonyan, Karen and Zisserman, Andrew},
 year = {04/09/2014},
 title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
 url = {https://arxiv.org/pdf/1409.1556},
 abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
 file = {Simonyan, Zisserman 04 09 2014 - Very Deep Convolutional Networks:Attachments/Simonyan, Zisserman 04 09 2014 - Very Deep Convolutional Networks.pdf:application/pdf}
}


@book{Srenson.1948,
 author = {S{\o}renson, T.},
 year = {1948},
 title = {A Method of Establishing Groups of Equal Amplitude in Plant Sociology Based on Similarity of Species Content and Its Application to Analyses of the Vegetation on Danish Commons},
 url = {https://books.google.de/books?id=rpS8GAAACAAJ},
 publisher = {{I kommission hos E. Munksgaard}},
 series = {Biologiske skrifter},
 abstract = {}
}


@misc{Tan.2020,
 author = {Tan, Weikai and Qin, Nannan and Ma, Lingfei and Li, Ying and Du, Jing and Cai, Guorong and Yang, Ke and Li, Jonathan},
 year = {2020},
 title = {Toronto-3D: A Large-scale Mobile LiDAR Dataset for Semantic Segmentation of Urban Roadways},
 url = {https://arxiv.org/pdf/2003.08284},
 abstract = {Semantic segmentation of large-scale outdoor point clouds is essential for urban scene understanding in various applications, especially autonomous driving and urban high-definition (HD) mapping. With rapid developments of mobile laser scanning (MLS) systems, massive point clouds are available for scene understanding, but publicly accessible large-scale labeled datasets, which are essential for developing learning-based methods, are still limited. This paper introduces Toronto-3D, a large-scale urban outdoor point cloud dataset acquired by a MLS system in Toronto, Canada for semantic segmentation. This dataset covers approximately 1 km of point clouds and consists of about 78.3 million points with 8 labeled object classes. Baseline experiments for semantic segmentation were conducted and the results confirmed the capability of this dataset to train deep learning models effectively. Toronto-3D is released to encourage new research, and the labels will be improved and updated with feedback from the research community.},
 doi = {10.1109/CVPRW50498.2020.00109},
 file = {Tan, Qin et al. 2020 - Toronto-3D:Attachments/Tan, Qin et al. 2020 - Toronto-3D.pdf:application/pdf}
}


@article{Yerram.2022,
 author = {Yerram, Varun and Takeshita, Hiroyuki and Iwahori, Yuji and Hayashi, Yoshitsugu and Bhuyan, M. K. and Fukui, Shinji and Kijsirikul, Boonserm and Wang, Aili},
 year = {2022},
 title = {Extraction and Calculation of Roadway Area from Satellite Images Using Improved Deep Learning Model and Post-Processing},
 pages = {124},
 pagination = {page},
 volume = {8},
 journaltitle = {Journal of Imaging},
 shortjournal = {J. Imaging},
 language = {eng},
 doi = {10.3390/jimaging8050124},
 number = {5},
 abstract = {Roadway area calculation is a novel problem in remote sensing and urban planning. This paper models this problem as a two-step problem, roadway extraction, and area calculation. Roadway extraction from satellite images is a problem that has been tackled many times before. This paper proposes a method using pixel resolution to calculate the area of the roads covered in satellite images. The proposed approach uses novel U-net and Resnet architectures called U-net++ and ResNeXt. The state-of-the-art model is combined with the proposed efficient post-processing approach to improve the overlap with ground truth labels. The performance of the proposed road extraction algorithm is evaluated on the Massachusetts dataset and it is shown that the proposed approach outperforms the existing solutions which use models from the U-net family.},
 file = {Yerram, Takeshita et al. 2022 - Extraction and Calculation of Roadway:Attachments/Yerram, Takeshita et al. 2022 - Extraction and Calculation of Roadway.pdf:application/pdf;Yerram, Takeshita et al. 2022 - Extraction and Calculation of Roadway (2):Attachments/Yerram, Takeshita et al. 2022 - Extraction and Calculation of Roadway (2).pdf:application/pdf},
 note = {PII:  jimaging8050124

Journal Article},
 eprint = {35621888}
}


@book{YutakaSasaki.2007,
 author = {{Yutaka Sasaki}},
 year = {2007},
 title = {The truth of the F-measure},
 url = {https://www.researchgate.net/publication/268185911_The_truth_of_the_F-measure},
 abstract = {PDF | It has been past more than 15 years since the F-measure was first introduced to evaluation tasks of information extraction technology at the... | Find, read and cite all the research you need on ResearchGate},
 file = {Yutaka Sasaki 2007 - The truth of the F-measure:Attachments/Yutaka Sasaki 2007 - The truth of the F-measure.pdf:application/pdf}
}


